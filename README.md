<p align="center">
  <img src="https://ciroh.ua.edu/wp-content/uploads/2022/08/CIROHLogo_200x200.png"
       alt="CIROH logo" width="20%" height="20%" />
</p>

<h1 align="center">ğŸ’§ CIROH Tethys Portal</h1>

![CIROH Portal â€“ Kubernetes Architecture](static/ciroh_portal_â€“_kubernetes_architecture.png)

> The **CIROH Portal** runs on an Amazon EKS cluster (`namespace: cirohportal`) and routes traffic like so:

* ğŸŒ **Users** â†’ **AWS Application Load Balancer** (managed by the *AWS Load Balancer Controller*).
* ğŸ“¦ The ALB forwards HTTP :`8080` to:
  * ğŸ–¥ï¸ `portal` Service â†’ CIROH web app.
  * ğŸ“ˆ `tds` Service â†’ THREDDS Data Server for large-file downloads.

Inside the namespace:

| Layer | Kubernetes Object | Purpose |
|-------|-------------------|---------|
| ğŸ–¥ï¸ **CIROH Portal** | Deployment â†’ Pod(s) | Django/Tethys web stack |
| ğŸ“ˆ **THREDDS** | Deployment â†’ Pod | Catalog + OPeNDAP downloads |
| ğŸ˜ **PostgreSQL** | Deployment â†’ Service :`5432` | User data & configuration |
| âš¡ **Redis** | StatefulSet â†’ Service :`6379` | Session & cache data |

Arrows in the diagram emphasise that **portal Pods talk to PostgreSQL (durable) and Redis (ephemeral)**, while **only the ALB is Internet-facing**â€”keeping private services private and cleanly separating compute, storage, and caching. ğŸ”’âœ¨

---

## ğŸ“š Built-in Apps

| # | App | Kind |
|---|-----|------|
| 1 | [Tethys Dash](https://github.com/tethysplatform/tethysapp-tethys_dash) | ğŸŒŠ Native |
| 2 | [Water Data Explorer](https://github.com/BYU-Hydroinformatics/Water-Data-Explorer) | ğŸŒŠ Native |
| 3 | [HydroCompute + HydroLang Demo](https://github.com/tethysplatform/tethysapp-hydrocompute) | ğŸŒŠ Native |
| 4 | [SWEML](https://github.com/karnesh/tethysapp-sweml) | ğŸŒŠ Native |
| 5 | [Ground Subsetting Tool](https://github.com/Aquaveo/ggst) | ğŸŒŠ Native |
| 6 | [Snow Inspector](https://github.com/BYU-Hydroinformatics/snow-inspector) | ğŸŒŠ Native |
| 7 | [CSES](https://github.com/whitelightning450/Tethys-CSES) | ğŸŒŠ Native |
| 8 | [NWM Map Viewer](https://water.noaa.gov/map) | ğŸŒ Proxy |
| 9 | [CIROH JupyterHub](https://jupyterhub.cuahsi.org/hub/login) | ğŸŒ Proxy |
| 10 | [HydroShare](https://www.hydroshare.org/home/) | ğŸŒ Proxy |
| 11 | [FIM Visualization](https://hydroinformatics.uiowa.edu/lab/fims/) | ğŸŒ Proxy |
| 12 | [Numerical Flash-Flood Alerts](https://numericalflashfloodalertsolutions.com/) | ğŸŒ Proxy |

---

## ğŸš€ Installation (on AWS)

> **Terraform + EKS = easy reproducible infra** ğŸª„

### 1 Â· Prerequisites ğŸ› ï¸

* ğŸ–¥ï¸ [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)
* ğŸ”‘ AWS credentials / named profile  
  (`~/.aws/credentials`)
* ğŸŒ [Terraform](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)

### 2 Â· Terraform Modules Structure ğŸ“‚

```bash
terraform/
â”œâ”€â”€ modules
â”‚   â””â”€â”€ ciroh_portal
â”‚       â”œâ”€â”€ alb.tf
â”‚       â”œâ”€â”€ efs.tf
â”‚       â”œâ”€â”€ eks.tf
â”‚       â”œâ”€â”€ iam-policy.json
â”‚       â”œâ”€â”€ karpenter.tf
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ metrics_server.tf
â”‚       â”œâ”€â”€ networking.tf
â”‚       â”œâ”€â”€ outputs.tf
â”‚       â”œâ”€â”€ scripts
â”‚       â”œâ”€â”€ tethys_portal.tf
â”‚       â””â”€â”€ variables.tf
â”œâ”€â”€ prod
â”‚   â”œâ”€â”€ backend.tf
â”‚   â””â”€â”€ main.tf
â””â”€â”€ README.md
```
Module variables you may tweak (region, cluster name, Helm chart â€¦)

**For example:**

```hcl
source = "../modules/ciroh_portal"
# Input Varibles
region              = "us-east-1"
profile             = "456531024327"
cluster_name        = "ciroh-portal-prod"
app_name            = "cirohportal"
helm_chart          = "ciroh"
helm_repo           = "https://docs.ciroh.org/tethysportal-ciroh"
helm_ci_path        = "../../charts/ciroh/ci"
environment         = "prod"
single_nat_gate_way = false
use_elastic_ips     = true
eips                = ["...", "..."]
```

### 3 Â· Deploy ğŸ“¦

In order to deploy the following commands can needs to be run:

**Configure kubectl**

```bash
# 1ï¸âƒ£ Configure kubectl
aws eks update-kubeconfig \
  --name <cluster_name> --region <region> --profile <profile>

# 2ï¸âƒ£ Terraform workflow
terraform init
terraform plan      # review ğŸ¤“
terraform apply     # ğŸš€

# (dev only) clean-up afterwards
terraform destroy
```

If the environment is a development one: the following code needs to be run after the dev env has been tested: `terraform destroy`

>**__NOTE__**: When terrafrom is deployed. It deploys the tethys portal using the helm provider. You need to be careful to check the the paths `charts/ciroh/ci/prod_aws_values.yaml` and `charts/ciroh/ci/dev_aws_values.yaml` depending on the environment that you are deploying to. You might need to check that you are using the correct configuration (docker image, etc)

### 4 Â· Upgrade Deployment only ğŸ”„ 

Once the deployment is completed, it is not necessary to deploy the whole infrastructure every time there is a change. If there is an update in the image that the portal is using the following command can be used:

```bash
# add the helm repo
helm repo add tethysportal-ciroh https://docs.ciroh.org/tethysportal-ciroh

helm upgrade cirohportal-prod tethysportal-ciroh/ciroh \
  --install --wait --timeout 3600 \
  -f charts/ciroh/ci/prod_aws_values.yaml \
  --set storageClass.parameters.fileSystemId=<EFS_ID> \
  --set image.tag=<newTag> \
  --namespace cirohportal
```


>**â— Remember:** You need to use the `--set storageClass.parameters.fileSystemId` to upgrade because it was not referenced when the chart was deployed with the terraform scripts. Therefore, you need to do it manually also for the upgrade. Similarly, in the values.yaml that you use you need to have `enabled: true` in the **StorageClass** section or it will produce an error.

If you upgrade fails with the following error:

```bash
Error: UPGRADE FAILED: another operation (install/upgrade/rollback) is in progress
```

you can rollback to a previous revision with:

```bash
helm history <release> --namespace <namespace>
helm rollback <release> <number_release> --namespace <namespace>
```

More on this in the following [medium article](https://medium.com/nerd-for-tech/kubernetes-helm-error-upgrade-failed-another-operation-install-upgrade-rollback-is-in-progress-52ea2c6fcda9)

### 5 Â· Troubleshooting ğŸ

- When the `terraform destroy` command does not work in one run, it can be du to a couple of reasons:
  - The ALB ingress gets destroy before, so the ingresses of the Tethys portal do nto get deleted. You might need to delete them in the AWS dashboard --> EC2--> Load Balancers
  - The VPC cannot be deleted, This is also realted to the ALB ingress being deleted before (leaving the ingresses of the Tethys Portal hanging around after the creation) As a result, the VPC is still being used by these Load Balancers. Delete the Load Balancers, and then the VPC in the AWS dashboard
- Deleting manually can sometimes cause the following error:
  - `Kubernetes cluster unreachable: invalid configuration: no configuration has been provided, try setting KUBERNETES_MASTER environment variable`
  - The following [Github Issue Comment](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/1234#issuecomment-894998800) is helpful
  - The following [Medium Article](https://itnext.io/terraform-dont-use-kubernetes-provider-with-your-cluster-resource-d8ec5319d14a) is helpful as well

### 6 Â· Handy Tools ğŸ”§

Monitoring cluster and deployments: [k9s](https://k9scli.io/)
Visualizing dynamic node usage within a cluster: [eks-node-viewer](https://github.com/awslabs/eks-node-viewer)

### 7 Â· Submodule Dev âœ¨

When updating git submodules the following can be useful: https://stackoverflow.com/questions/29882960/changing-an-existing-submodules-branch
